# Image-to-text
The presented code implements a solution for the "Image-to-Text" problem using an encoder-decoder based architecture with attention mechanisms. This approach makes use of convolutional networks (CNNs) and attention to process images and generate textual regularities.
![Exemplo do dataset](https://miro.medium.com/max/1400/1*6BFOIdSHlk24Z3DFEakvnQ.png)
## Implementation Summary

### Encoder

The encoder employs the pre-trained EfficientNet as a foundation for the extraction of features from images. The final layers of the EfficientNet, which are responsible for classification, are removed, leaving only the convolutional layers. The aforementioned layers generate a feature map with dimensions (1028, 7, 7), which represents the spatial information of the image.

Subsequently, the aforementioned features are projected to a smaller vector space through the implementation of a 1x1 convolution, after which they are reordered into a sequence of vectors. Each vector thus represents a discrete concept or "word" within the image. The sequence is then subjected to a series of attention layers (utilising the `EncoderAttention` class), which serve to refine the features and facilitate the capture of contextual information.

### **Decoder**

While the decoder code was not included in the presented section, it would typically be responsible for taking the sequence generated by the encoder and, through several layers of attention and embedding, generating the sequence of words that describe the image. The decoder can be implemented in a similar manner to the encoder, with a greater emphasis on text generation. This is achieved through the use of cross-attention layers, which correlate the features of the image with the text tokens.

The main components of the system are as follows:
1. EfficientNet: The initial extraction of features from the images is the responsibility of this component.

2. A 1x1 convolution is a mathematical operation that reduces the dimensions of the features extracted from the image. This process reduces the dimensionality of the extracted features from EfficientNet.

3. Positional Embeddings The addition of positional information to the features serves to maintain the order of the sequence.

4. The attention layers are as follows: The features are refined and contextual information is captured.

## **Data Processing**

The data set employed is the Flickr8k, comprising 8,000 images with textual descriptions. The descriptions are tokenised, converted to lowercase, and any special characters are removed. Special tokens, including `<START>`, `<END>`, `<UNK>`, and `<PAD>`, are then added. Each caption is transformed into a sequence of identifiers, which represent the index of the words in the vocabulary.

## Conclusion

The proposed model employs advanced computational vision and natural language processing techniques to transform images into textual descriptions. The encoder-decoder architecture with attention mechanism enables the capture of the requisite contextual information for the generation of accurate and comprehensible descriptions. This is achieved through the utilisation of EfficientNet for feature extraction and attention mechanisms for sequential processing.
Should assistance be required with the decoder or additional optimisations, I am available to provide support.
